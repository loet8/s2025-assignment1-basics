{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loet8/s2025-assignment1-basics/blob/main/Training_TS_and_OWT_ECE496B_Homework1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijR6qPINU_P6",
        "outputId": "eaf70af2-c55d-4155-97b1-32603d7a87ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 's2025-assignment1-basics'...\n",
            "remote: Enumerating objects: 225, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 225 (delta 43), reused 39 (delta 39), pack-reused 173 (from 1)\u001b[K\n",
            "Receiving objects: 100% (225/225), 11.89 MiB | 11.19 MiB/s, done.\n",
            "Resolving deltas: 100% (105/105), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/loet8/s2025-assignment1-basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5eKjGhFMiZ6",
        "outputId": "70174ab1-415c-4550-aa4c-66e0b71ce726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/s2025-assignment1-basics/s2025-assignment1-basics\n"
          ]
        }
      ],
      "source": [
        "%cd s2025-assignment1-basics/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVh85UKdMng8",
        "outputId": "f66265b1-1596-40bd-9fcd-51e3f38f9667"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLdwVyZ3MKk8",
        "outputId": "4ae264d3-dc0d-497d-b172-74f2d3c7c1cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already on 'main'\n",
            "Your branch is up to date with 'origin/main'.\n"
          ]
        }
      ],
      "source": [
        "!git checkout main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUpCszLnfDf1"
      },
      "source": [
        "No need for Conda environment on Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1tPK7iKenBW",
        "outputId": "36e73bc7-ab94-4dfa-dca0-16bbe15d4ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/s2025-assignment1-basics/s2025-assignment1-basics\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from ece496b_basics==0.1.7.dev0) (2024.11.6)\n",
            "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.11/dist-packages (from ece496b_basics==0.1.7.dev0) (2.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ece496b_basics==0.1.7.dev0) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1->ece496b_basics==0.1.7.dev0) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->ece496b_basics==0.1.7.dev0) (12.5.82)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from ece496b_basics==0.1.7.dev0) (8.3.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from ece496b_basics==0.1.7.dev0) (0.9.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ece496b_basics==0.1.7.dev0) (5.9.5)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->ece496b_basics==0.1.7.dev0) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest->ece496b_basics==0.1.7.dev0) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->ece496b_basics==0.1.7.dev0) (1.5.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->ece496b_basics==0.1.7.dev0) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->ece496b_basics==0.1.7.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->ece496b_basics==0.1.7.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->ece496b_basics==0.1.7.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->ece496b_basics==0.1.7.dev0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.1->ece496b_basics==0.1.7.dev0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.1->ece496b_basics==0.1.7.dev0) (1.3.0)\n",
            "Installing collected packages: ece496b_basics\n",
            "  Attempting uninstall: ece496b_basics\n",
            "    Found existing installation: ece496b_basics 0.1.7.dev0\n",
            "    Uninstalling ece496b_basics-0.1.7.dev0:\n",
            "      Successfully uninstalled ece496b_basics-0.1.7.dev0\n",
            "  Running setup.py develop for ece496b_basics\n",
            "Successfully installed ece496b_basics-0.1.7.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install -e .'[test]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-_dABLeW3LR",
        "outputId": "c81072cb-27ea-4bfb-c6ba-7e624087e14e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.4, pluggy-1.5.0\n",
            "rootdir: /content/s2025-assignment1-basics/s2025-assignment1-basics\n",
            "configfile: pytest.ini\n",
            "plugins: langsmith-0.3.8, anyio-3.7.1, typeguard-4.4.1\n",
            "collected 2 items                                                                                  \u001b[0m\n",
            "\n",
            "tests/test_train_bpe.py::test_train_bpe_speed \u001b[32mPASSED\u001b[0m\u001b[32m                                         [ 50%]\u001b[0m\n",
            "tests/test_train_bpe.py::test_train_bpe \u001b[32mPASSED\u001b[0m\u001b[32m                                               [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 3.95s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pytest tests/test_train_bpe.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest"
      ],
      "metadata": {
        "id": "5noYchsLuqVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TDqd5Btrji4F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25475e0b-d8aa-42d3-f775-60ff25d55d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/s2025-assignment1-basics\n",
            "/content/s2025-assignment1-basics/data\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "%mkdir -p data\n",
        "%cd data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wrA646Gf3Ht",
        "outputId": "32d245e3-a48f-46aa-fe5e-1d051e0e37b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-16 07:26:41--  https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.11, 3.165.160.12, 3.165.160.61, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/42/7f/427f7497b6c6596c18b46d5a72e61364fcad12aa433c60a0dbd4d344477b9d81/6418d412de72888f52b5142c761ac21a582f7d1166f0bfbdb5f03ccfdec90443?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27TinyStoriesV2-GPT4-train.txt%3B+filename%3D%22TinyStoriesV2-GPT4-train.txt%22%3B&response-content-type=text%2Fplain&Expires=1739694401&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTY5NDQwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Mi83Zi80MjdmNzQ5N2I2YzY1OTZjMThiNDZkNWE3MmU2MTM2NGZjYWQxMmFhNDMzYzYwYTBkYmQ0ZDM0NDQ3N2I5ZDgxLzY0MThkNDEyZGU3Mjg4OGY1MmI1MTQyYzc2MWFjMjFhNTgyZjdkMTE2NmYwYmZiZGI1ZjAzY2NmZGVjOTA0NDM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=b3miuA421ZbA3ZxQayC0H91NToNXj95B5AvtgOc2nx3CeLUdX3vq%7ERLIK3Vt32Is0vzWmwOsvK%7EpFTzO589Q4exWj3A%7EtYGgxGZHAOqjE9lJUEd-9tOLjFjzAkPcwSRz0VmuRh5BiBRSm9E%7EaGzxia6NfGcXTsXSURj8Bg-68pUu9tsLHEIntxyanwlbaTlK4oHswomn0is86cmK7P7XCUpYHxjxQIRR7TAMR-lEVWhluIUWvhEVhrd4p7axvRhtk-npJTLwpYPFN47RNiWbsitroo4CDgiPc5IJhOVa8YxD1T4G%7E13uR48bkogSVvX7Xtp5RN-bLeJ839VXPr3EVg__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-02-16 07:26:41--  https://cdn-lfs.hf.co/repos/42/7f/427f7497b6c6596c18b46d5a72e61364fcad12aa433c60a0dbd4d344477b9d81/6418d412de72888f52b5142c761ac21a582f7d1166f0bfbdb5f03ccfdec90443?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27TinyStoriesV2-GPT4-train.txt%3B+filename%3D%22TinyStoriesV2-GPT4-train.txt%22%3B&response-content-type=text%2Fplain&Expires=1739694401&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTY5NDQwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Mi83Zi80MjdmNzQ5N2I2YzY1OTZjMThiNDZkNWE3MmU2MTM2NGZjYWQxMmFhNDMzYzYwYTBkYmQ0ZDM0NDQ3N2I5ZDgxLzY0MThkNDEyZGU3Mjg4OGY1MmI1MTQyYzc2MWFjMjFhNTgyZjdkMTE2NmYwYmZiZGI1ZjAzY2NmZGVjOTA0NDM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=b3miuA421ZbA3ZxQayC0H91NToNXj95B5AvtgOc2nx3CeLUdX3vq%7ERLIK3Vt32Is0vzWmwOsvK%7EpFTzO589Q4exWj3A%7EtYGgxGZHAOqjE9lJUEd-9tOLjFjzAkPcwSRz0VmuRh5BiBRSm9E%7EaGzxia6NfGcXTsXSURj8Bg-68pUu9tsLHEIntxyanwlbaTlK4oHswomn0is86cmK7P7XCUpYHxjxQIRR7TAMR-lEVWhluIUWvhEVhrd4p7axvRhtk-npJTLwpYPFN47RNiWbsitroo4CDgiPc5IJhOVa8YxD1T4G%7E13uR48bkogSVvX7Xtp5RN-bLeJ839VXPr3EVg__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.169.231.4, 3.169.231.38, 3.169.231.115, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.169.231.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2227753162 (2.1G) [text/plain]\n",
            "Saving to: ‘TinyStoriesV2-GPT4-train.txt’\n",
            "\n",
            "TinyStoriesV2-GPT4- 100%[===================>]   2.07G   230MB/s    in 10s     \n",
            "\n",
            "2025-02-16 07:26:51 (210 MB/s) - ‘TinyStoriesV2-GPT4-train.txt’ saved [2227753162/2227753162]\n",
            "\n",
            "--2025-02-16 07:26:51--  https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.11, 3.165.160.12, 3.165.160.61, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/42/7f/427f7497b6c6596c18b46d5a72e61364fcad12aa433c60a0dbd4d344477b9d81/6874bae9a4c1a4e7edcf0e53b86c17817e9cf881fc75ff2368da457b80c0585d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27TinyStoriesV2-GPT4-valid.txt%3B+filename%3D%22TinyStoriesV2-GPT4-valid.txt%22%3B&response-content-type=text%2Fplain&Expires=1739694411&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTY5NDQxMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Mi83Zi80MjdmNzQ5N2I2YzY1OTZjMThiNDZkNWE3MmU2MTM2NGZjYWQxMmFhNDMzYzYwYTBkYmQ0ZDM0NDQ3N2I5ZDgxLzY4NzRiYWU5YTRjMWE0ZTdlZGNmMGU1M2I4NmMxNzgxN2U5Y2Y4ODFmYzc1ZmYyMzY4ZGE0NTdiODBjMDU4NWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=WaaeeG1ClTpmPUcO6ji5TNJirwSuvujV7%7EWicFN4sW5WZjBR-9THeGfwh8Er7uuL53qqZTH7YNZmLg7z4xpMWKbNe3d%7EKmmYu56n5NEEL1wHlMeyv5xo2hJN4Cl4ZuXrDF0H-MquyoaSdX55-QGzeYzhr8vBgN9w1IUDI1ebAIiJXx%7Eljev6a2Hw6-kd5JDZCYl9Gvlz4rgW03j%7E50fVnj3rM%7EV5zJhxQtK8zzHpVQDGZFfK8KG2BrG5URnqcF6hjHJTEftKgWny4Q5PNK0-14NM1Ge-QcOZFgUNB83DSEqk%7ExQaSpINhuP0utJx7ZaQHYhan0dHlmCH1JRPeNSFFg__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-02-16 07:26:51--  https://cdn-lfs.hf.co/repos/42/7f/427f7497b6c6596c18b46d5a72e61364fcad12aa433c60a0dbd4d344477b9d81/6874bae9a4c1a4e7edcf0e53b86c17817e9cf881fc75ff2368da457b80c0585d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27TinyStoriesV2-GPT4-valid.txt%3B+filename%3D%22TinyStoriesV2-GPT4-valid.txt%22%3B&response-content-type=text%2Fplain&Expires=1739694411&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTY5NDQxMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Mi83Zi80MjdmNzQ5N2I2YzY1OTZjMThiNDZkNWE3MmU2MTM2NGZjYWQxMmFhNDMzYzYwYTBkYmQ0ZDM0NDQ3N2I5ZDgxLzY4NzRiYWU5YTRjMWE0ZTdlZGNmMGU1M2I4NmMxNzgxN2U5Y2Y4ODFmYzc1ZmYyMzY4ZGE0NTdiODBjMDU4NWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=WaaeeG1ClTpmPUcO6ji5TNJirwSuvujV7%7EWicFN4sW5WZjBR-9THeGfwh8Er7uuL53qqZTH7YNZmLg7z4xpMWKbNe3d%7EKmmYu56n5NEEL1wHlMeyv5xo2hJN4Cl4ZuXrDF0H-MquyoaSdX55-QGzeYzhr8vBgN9w1IUDI1ebAIiJXx%7Eljev6a2Hw6-kd5JDZCYl9Gvlz4rgW03j%7E50fVnj3rM%7EV5zJhxQtK8zzHpVQDGZFfK8KG2BrG5URnqcF6hjHJTEftKgWny4Q5PNK0-14NM1Ge-QcOZFgUNB83DSEqk%7ExQaSpINhuP0utJx7ZaQHYhan0dHlmCH1JRPeNSFFg__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.169.231.4, 3.169.231.38, 3.169.231.115, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.169.231.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22502601 (21M) [text/plain]\n",
            "Saving to: ‘TinyStoriesV2-GPT4-valid.txt’\n",
            "\n",
            "TinyStoriesV2-GPT4- 100%[===================>]  21.46M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-02-16 07:26:52 (155 MB/s) - ‘TinyStoriesV2-GPT4-valid.txt’ saved [22502601/22502601]\n",
            "\n",
            "--2025-02-16 07:26:52--  https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_train.txt.gz\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.11, 3.165.160.12, 3.165.160.61, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/56/35/56359101d2f3062d0e2aef6c02f65e54f35b2c4e53eaf92f8116bf2877b7df2f/b19ae88cfbc4016b304c348522455fe38ebac48fffed955adcc7191a89e38ccf?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27owt_train.txt.gz%3B+filename%3D%22owt_train.txt.gz%22%3B&response-content-type=application%2Fgzip&Expires=1739694412&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTY5NDQxMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU2LzM1LzU2MzU5MTAxZDJmMzA2MmQwZTJhZWY2YzAyZjY1ZTU0ZjM1YjJjNGU1M2VhZjkyZjgxMTZiZjI4NzdiN2RmMmYvYjE5YWU4OGNmYmM0MDE2YjMwNGMzNDg1MjI0NTVmZTM4ZWJhYzQ4ZmZmZWQ5NTVhZGNjNzE5MWE4OWUzOGNjZj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=ucKeDT7Lcpsjh3uOAvAkn5zHsp1BHb1bFOO9UXFFAJVbcQaaKlxVa8753VK-5WA5U4B5u85YDaRp2n1xNvumRZFfKU8z%7EfPHbvVyczMGWydgfBnEp4MCjh%7ElSIEsh%7Egic3DudqndqYAUk3r52BvaAUmGTzoDDURA14Du8XVCCU5D3YwLInUd0EvgfAI6ffbQqz3SWXc5Qzhwtv9iJ5-Jw7b4uAA1PBjNAP7oi3ewS7aN4fYB3Sf8qfkPRvm%7E7F-UlVm8C%7E58dVZkzypIMmJcRKTcjIaNNcLGh%7EkJfHfYyGKmYp%7ED%7Ese7bQBMPo5tez4xoNbfl0WiiExizun1L8xRYQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-02-16 07:26:52--  https://cdn-lfs-us-1.hf.co/repos/56/35/56359101d2f3062d0e2aef6c02f65e54f35b2c4e53eaf92f8116bf2877b7df2f/b19ae88cfbc4016b304c348522455fe38ebac48fffed955adcc7191a89e38ccf?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27owt_train.txt.gz%3B+filename%3D%22owt_train.txt.gz%22%3B&response-content-type=application%2Fgzip&Expires=1739694412&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTY5NDQxMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU2LzM1LzU2MzU5MTAxZDJmMzA2MmQwZTJhZWY2YzAyZjY1ZTU0ZjM1YjJjNGU1M2VhZjkyZjgxMTZiZjI4NzdiN2RmMmYvYjE5YWU4OGNmYmM0MDE2YjMwNGMzNDg1MjI0NTVmZTM4ZWJhYzQ4ZmZmZWQ5NTVhZGNjNzE5MWE4OWUzOGNjZj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=ucKeDT7Lcpsjh3uOAvAkn5zHsp1BHb1bFOO9UXFFAJVbcQaaKlxVa8753VK-5WA5U4B5u85YDaRp2n1xNvumRZFfKU8z%7EfPHbvVyczMGWydgfBnEp4MCjh%7ElSIEsh%7Egic3DudqndqYAUk3r52BvaAUmGTzoDDURA14Du8XVCCU5D3YwLInUd0EvgfAI6ffbQqz3SWXc5Qzhwtv9iJ5-Jw7b4uAA1PBjNAP7oi3ewS7aN4fYB3Sf8qfkPRvm%7E7F-UlVm8C%7E58dVZkzypIMmJcRKTcjIaNNcLGh%7EkJfHfYyGKmYp%7ED%7Ese7bQBMPo5tez4xoNbfl0WiiExizun1L8xRYQ__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.164.174.19, 18.164.174.52, 18.164.174.98, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.164.174.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4591240837 (4.3G) [application/gzip]\n",
            "Saving to: ‘owt_train.txt.gz’\n",
            "\n",
            "owt_train.txt.gz    100%[===================>]   4.28G  40.7MB/s    in 1m 49s  \n",
            "\n",
            "2025-02-16 07:28:41 (40.1 MB/s) - ‘owt_train.txt.gz’ saved [4591240837/4591240837]\n",
            "\n",
            "--2025-02-16 07:31:09--  https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_valid.txt.gz\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.118, 18.164.174.23, 18.164.174.55, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/56/35/56359101d2f3062d0e2aef6c02f65e54f35b2c4e53eaf92f8116bf2877b7df2f/bc73db5da2f19c360836b9c8a88094e13346ec83798c2f40060be39135768c80?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27owt_valid.txt.gz%3B+filename%3D%22owt_valid.txt.gz%22%3B&response-content-type=application%2Fgzip&Expires=1739694669&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTY5NDY2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU2LzM1LzU2MzU5MTAxZDJmMzA2MmQwZTJhZWY2YzAyZjY1ZTU0ZjM1YjJjNGU1M2VhZjkyZjgxMTZiZjI4NzdiN2RmMmYvYmM3M2RiNWRhMmYxOWMzNjA4MzZiOWM4YTg4MDk0ZTEzMzQ2ZWM4Mzc5OGMyZjQwMDYwYmUzOTEzNTc2OGM4MD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=nK2PebNG2sZNZqYGJRQfHVQrJit-6hE0ecWPPdpYzOxKHqsAsuzVWwbAJRQRxZyk0cATgshKTaKv7Wip8KIulcGVCOW8t4vuk-ouh1w0IEBgotS7K8mSzIxx1y1vmq7NUHi4ZStpj6f3mVtrGHVuCigsn6APyaRJz7Cehz6Nu-frCZziDjjMNzLtIndngPXTsUL%7EwUiQxSRK5Ki2SuoUKWmSGCMK8zWE4XdNVWj-whwaXTvJdgnQnY6aoxUIsEcgvalpVNcTMlz4n7CX5-hjCD%7EDr4Qt6tCMVryfzOwKVFcPbDzCOZD4BhWoE4VxmfFUUcb4OFdzWsu7%7EfwrTORaLQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-02-16 07:31:09--  https://cdn-lfs-us-1.hf.co/repos/56/35/56359101d2f3062d0e2aef6c02f65e54f35b2c4e53eaf92f8116bf2877b7df2f/bc73db5da2f19c360836b9c8a88094e13346ec83798c2f40060be39135768c80?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27owt_valid.txt.gz%3B+filename%3D%22owt_valid.txt.gz%22%3B&response-content-type=application%2Fgzip&Expires=1739694669&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTY5NDY2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU2LzM1LzU2MzU5MTAxZDJmMzA2MmQwZTJhZWY2YzAyZjY1ZTU0ZjM1YjJjNGU1M2VhZjkyZjgxMTZiZjI4NzdiN2RmMmYvYmM3M2RiNWRhMmYxOWMzNjA4MzZiOWM4YTg4MDk0ZTEzMzQ2ZWM4Mzc5OGMyZjQwMDYwYmUzOTEzNTc2OGM4MD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=nK2PebNG2sZNZqYGJRQfHVQrJit-6hE0ecWPPdpYzOxKHqsAsuzVWwbAJRQRxZyk0cATgshKTaKv7Wip8KIulcGVCOW8t4vuk-ouh1w0IEBgotS7K8mSzIxx1y1vmq7NUHi4ZStpj6f3mVtrGHVuCigsn6APyaRJz7Cehz6Nu-frCZziDjjMNzLtIndngPXTsUL%7EwUiQxSRK5Ki2SuoUKWmSGCMK8zWE4XdNVWj-whwaXTvJdgnQnY6aoxUIsEcgvalpVNcTMlz4n7CX5-hjCD%7EDr4Qt6tCMVryfzOwKVFcPbDzCOZD4BhWoE4VxmfFUUcb4OFdzWsu7%7EfwrTORaLQ__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.164.174.52, 18.164.174.97, 18.164.174.19, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.164.174.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 111785382 (107M) [application/gzip]\n",
            "Saving to: ‘owt_valid.txt.gz’\n",
            "\n",
            "owt_valid.txt.gz    100%[===================>] 106.61M  40.5MB/s    in 2.6s    \n",
            "\n",
            "2025-02-16 07:31:11 (40.5 MB/s) - ‘owt_valid.txt.gz’ saved [111785382/111785382]\n",
            "\n",
            "/content/s2025-assignment1-basics\n",
            "/content/s2025-assignment1-basics/s2025-assignment1-basics\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt\n",
        "!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt\n",
        "\n",
        "!wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_train.txt.gz\n",
        "!gunzip owt_train.txt.gz\n",
        "!wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_valid.txt.gz\n",
        "!gunzip owt_valid.txt.gz\n",
        "\n",
        "%cd ..\n",
        "%cd s2025-assignment1-basics/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkuHdnn-caO_",
        "outputId": "7ccf9631-0e06-4bf0-8594-d0458f3c023a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/s2025-assignment1-basics\n",
            "adapters.py\t\t\t\t ece496b_basics.egg-info  requirements-test.txt\n",
            "CHANGELOG.md\t\t\t\t learning_rate_tuning.py  requirements.txt\n",
            "cs336_spring2024_assignment1_basics.pdf  __pycache__\t\t  s2025-assignment1-basics\n",
            "data\t\t\t\t\t pytest.ini\t\t  setup.py\n",
            "ece496b_basics\t\t\t\t README.md\t\t  tests\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "!ls\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXgj2lP_eYQl",
        "outputId": "311b2051-3e3b-4fc1-99db-db49e023a7c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting streaming BPE training...\n",
            "Processing batch 1...\n",
            "Processing batch 2...\n",
            "Processing batch 3...\n",
            "Processing batch 4...\n",
            "Processing batch 5...\n",
            "Processing batch 6...\n",
            "Processing batch 7...\n",
            "Processing batch 8...\n",
            "Processing batch 9...\n",
            "Processing batch 10...\n",
            "Training complete!\n",
            "Saving vocabulary and merges... (Elapsed Time: 53.49s)\n",
            "Saved vocabulary to /content/s2025-assignment1-basics/data/tinystories_bpe_vocab.txt\n",
            "Saved merges to /content/s2025-assignment1-basics/data/tinystories_bpe_merges.txt\n",
            "\n",
            "Longest Token:  enthusiastically\n",
            "Token ID: 7559\n",
            "Length: 17 bytes\n",
            "Loading trained tokenizer...\n",
            "\n",
            "Encoded: [1177, 1520, 44, 769, 486, 348, 63]\n",
            "Decoded: Hello world, how are you?\n",
            "Encoding-Decoding Test Passed!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import tempfile\n",
        "import  adapters\n",
        "\n",
        "REPO_DIR = \"/content/s2025-assignment1-basics\"\n",
        "DATA_DIR = os.path.join(REPO_DIR, \"data\")  # Define DATA_DIR here\n",
        "\n",
        "TINYSTORIES_VOCAB_PATH = os.path.join(DATA_DIR, \"tinystories_bpe_vocab.txt\")\n",
        "TINYSTORIES_MERGES_PATH = os.path.join(DATA_DIR, \"tinystories_bpe_merges.txt\")\n",
        "MERGES_PATH = os.path.join(DATA_DIR, \"bpe_merges.txt\")\n",
        "INPUT_PATH = os.path.join(DATA_DIR, \"TinyStoriesV2-GPT4-train.txt\")\n",
        "\n",
        "VOCAB_SIZE = 10000\n",
        "SPECIAL_TOKENS = [\"<|endoftext|>\"]\n",
        "BATCH_SIZE = 10000\n",
        "MAX_LINES = 100000\n",
        "\n",
        "def train_bpe_streaming(input_path, vocab_size, special_tokens, batch_size, max_lines):\n",
        "    print(\"Starting streaming BPE training...\")\n",
        "    vocab, merges = None, []\n",
        "\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        line_count = 0\n",
        "        batch_data = []\n",
        "\n",
        "        for line in file:\n",
        "            batch_data.append(line)\n",
        "            line_count += 1\n",
        "\n",
        "            if line_count % batch_size == 0:\n",
        "                print(f\"Processing batch {line_count // batch_size}...\")\n",
        "\n",
        "                with tempfile.NamedTemporaryFile(mode=\"w+\", delete=False) as tmp_file:\n",
        "                    tmp_file.writelines(batch_data)\n",
        "                    tmp_file.flush()\n",
        "                    batch_data.clear()\n",
        "\n",
        "                    vocab, merges = adapters.run_train_bpe(tmp_file.name, vocab_size, special_tokens)\n",
        "\n",
        "                os.remove(tmp_file.name)\n",
        "\n",
        "            if line_count >= max_lines:\n",
        "                break\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    return vocab, merges\n",
        "\n",
        "start_time = time.time()\n",
        "vocab, merges = train_bpe_streaming(INPUT_PATH, VOCAB_SIZE, SPECIAL_TOKENS, BATCH_SIZE, MAX_LINES)\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print(f\"Saving vocabulary and merges... (Elapsed Time: {elapsed_time:.2f}s)\")\n",
        "with open(TINYSTORIES_VOCAB_PATH, \"w\", encoding=\"utf-8\") as vf:\n",
        "    for k, v in vocab.items():\n",
        "        vf.write(f\"{k} {v.hex()}\\n\")\n",
        "\n",
        "with open(TINYSTORIES_MERGES_PATH, \"w\", encoding=\"utf-8\") as mf:\n",
        "    for merge in merges:\n",
        "        mf.write(f\"{merge[0].hex()} {merge[1].hex()}\\n\")\n",
        "\n",
        "\n",
        "print(f\"Saved vocabulary to {TINYSTORIES_VOCAB_PATH}\")\n",
        "print(f\"Saved merges to {TINYSTORIES_MERGES_PATH}\\n\")\n",
        "\n",
        "longest_token = max(vocab.values(), key=len)\n",
        "longest_token_id = [k for k, v in vocab.items() if v == longest_token][0]\n",
        "\n",
        "print(f\"Longest Token: {longest_token.decode('utf-8', errors='ignore')}\")\n",
        "print(f\"Token ID: {longest_token_id}\")\n",
        "print(f\"Length: {len(longest_token)} bytes\")\n",
        "\n",
        "\n",
        "print(\"Loading trained tokenizer...\")\n",
        "ts_tokenizer = adapters.get_tokenizer(vocab, merges, SPECIAL_TOKENS)\n",
        "\n",
        "sample_text = \"Hello world, how are you?\"\n",
        "encoded = ts_tokenizer.encode(sample_text)\n",
        "decoded = ts_tokenizer.decode(encoded)\n",
        "\n",
        "print(f\"\\nEncoded: {encoded}\")\n",
        "print(f\"Decoded: {decoded}\")\n",
        "\n",
        "if sample_text == decoded:\n",
        "    print(\"Encoding-Decoding Test Passed!\")\n",
        "else:\n",
        "    print(\"Encoding-Decoding Mismatch!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwGjDvmJbCjy",
        "outputId": "63a3da2d-c296-4876-d04e-f20ce9ca85a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting streaming BPE training...\n",
            "Processing batch 1...\n",
            "Processing batch 2...\n",
            "Processing batch 3...\n",
            "Processing batch 4...\n",
            "Processing batch 5...\n",
            "Processing batch 6...\n",
            "Processing batch 7...\n",
            "Processing batch 8...\n",
            "Processing batch 9...\n",
            "Processing batch 10...\n",
            "Training complete!\n",
            "Saving vocabulary and merges... (Elapsed Time: 1347.34s)\n",
            "Saved vocabulary to /content/s2025-assignment1-basics/data/owt_bpe_vocab.txt\n",
            "Saved merges to /content/s2025-assignment1-basics/data/owt_bpe_merges.txt\n",
            "\n",
            "Longest Token: ______________________________________________________\n",
            "Token ID: 26683\n",
            "Length: 54 bytes\n",
            "Loading trained tokenizer...\n",
            "\n",
            "Encoded: [17936, 872, 44, 742, 373, 346, 63]\n",
            "Decoded: Hello world, how are you?\n",
            "Encoding-Decoding Test Passed!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import tempfile\n",
        "import adapters\n",
        "\n",
        "REPO_DIR = \"/content/s2025-assignment1-basics\"\n",
        "DATA_DIR = os.path.join(REPO_DIR, \"data\")\n",
        "VOCAB_PATH = os.path.join(DATA_DIR, \"owt_bpe_vocab.txt\")\n",
        "MERGES_PATH = os.path.join(DATA_DIR, \"owt_bpe_merges.txt\")\n",
        "INPUT_PATH = os.path.join(DATA_DIR, \"owt_train.txt\")\n",
        "\n",
        "\n",
        "VOCAB_SIZE = 32000\n",
        "SPECIAL_TOKENS = [\"<|endoftext|>\"]\n",
        "BATCH_SIZE = 10000\n",
        "MAX_LINES = 100000\n",
        "\n",
        "def train_bpe_streaming(input_path, vocab_size, special_tokens, batch_size, max_lines):\n",
        "    print(\"Starting streaming BPE training...\")\n",
        "    vocab, merges = None, []\n",
        "\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        line_count = 0\n",
        "        batch_data = []\n",
        "\n",
        "        for line in file:\n",
        "            batch_data.append(line)\n",
        "            line_count += 1\n",
        "\n",
        "            if line_count % batch_size == 0:\n",
        "                print(f\"Processing batch {line_count // batch_size}...\")\n",
        "\n",
        "                with tempfile.NamedTemporaryFile(mode=\"w+\", delete=False) as tmp_file:\n",
        "                    tmp_file.writelines(batch_data)\n",
        "                    tmp_file.flush()\n",
        "                    batch_data.clear()\n",
        "\n",
        "                    vocab, merges = adapters.run_train_bpe(tmp_file.name, vocab_size, special_tokens)\n",
        "\n",
        "                os.remove(tmp_file.name)\n",
        "\n",
        "            if line_count >= max_lines:\n",
        "                break\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    return vocab, merges\n",
        "\n",
        "start_time = time.time()\n",
        "vocab, merges = train_bpe_streaming(INPUT_PATH, VOCAB_SIZE, SPECIAL_TOKENS, BATCH_SIZE, MAX_LINES)\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print(f\"Saving vocabulary and merges... (Elapsed Time: {elapsed_time:.2f}s)\")\n",
        "with open(VOCAB_PATH, \"w\", encoding=\"utf-8\") as vf:\n",
        "    for k, v in vocab.items():\n",
        "        vf.write(f\"{k} {v.hex()}\\n\")\n",
        "\n",
        "with open(MERGES_PATH, \"w\", encoding=\"utf-8\") as mf:\n",
        "    for merge in merges:\n",
        "        mf.write(f\"{merge[0].hex()} {merge[1].hex()}\\n\")\n",
        "\n",
        "print(f\"Saved vocabulary to {VOCAB_PATH}\")\n",
        "print(f\"Saved merges to {MERGES_PATH}\\n\")\n",
        "\n",
        "\n",
        "longest_token = max(vocab.values(), key=len)\n",
        "longest_token_id = [k for k, v in vocab.items() if v == longest_token][0]\n",
        "\n",
        "print(f\"Longest Token: {longest_token.decode('utf-8', errors='ignore')}\")\n",
        "print(f\"Token ID: {longest_token_id}\")\n",
        "print(f\"Length: {len(longest_token)} bytes\")\n",
        "\n",
        "\n",
        "print(\"Loading trained tokenizer...\")\n",
        "owt_tokenizer = adapters.get_tokenizer(vocab, merges, SPECIAL_TOKENS)\n",
        "\n",
        "sample_text = \"Hello world, how are you?\"\n",
        "encoded = owt_tokenizer.encode(sample_text)\n",
        "decoded = owt_tokenizer.decode(encoded)\n",
        "\n",
        "print(f\"\\nEncoded: {encoded}\")\n",
        "print(f\"Decoded: {decoded}\")\n",
        "\n",
        "if sample_text == decoded:\n",
        "    print(\"Encoding-Decoding Test Passed!\")\n",
        "else:\n",
        "    print(\"Encoding-Decoding Mismatch!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omklDyvjitU7",
        "outputId": "f241f280-7a64-4c0b-c314-32a6bdf8fea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 14G\n",
            "-rw-r--r-- 1 root root 477K Feb 16 08:04 owt_bpe_merges.txt\n",
            "-rw-r--r-- 1 root root 623K Feb 16 08:04 owt_bpe_vocab.txt\n",
            "-rw-r--r-- 1 root root  12G Apr  1  2024 owt_train.txt\n",
            "-rw-r--r-- 1 root root 277M Apr  1  2024 owt_valid.txt\n",
            "-rw-r--r-- 1 root root 104K Feb 16 07:35 tinystories_bpe_merges.txt\n",
            "-rw-r--r-- 1 root root 135K Feb 16 07:35 tinystories_bpe_vocab.txt\n",
            "-rw-r--r-- 1 root root 2.1G May 19  2023 TinyStoriesV2-GPT4-train.txt\n",
            "-rw-r--r-- 1 root root  22M May 19  2023 TinyStoriesV2-GPT4-valid.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -lh /content/s2025-assignment1-basics/data/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "286ibNuyiOBq",
        "outputId": "f05a059e-9d5b-4078-c7c2-3f60e40816bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer from /content/s2025-assignment1-basics/data/tinystories_bpe_vocab.txt and /content/s2025-assignment1-basics/data/tinystories_bpe_merges.txt...\n",
            "Tokenizer loaded successfully from /content/s2025-assignment1-basics/data/tinystories_bpe_vocab.txt\n",
            "Loading tokenizer from /content/s2025-assignment1-basics/data/owt_bpe_vocab.txt and /content/s2025-assignment1-basics/data/owt_bpe_merges.txt...\n",
            "Tokenizer loaded successfully from /content/s2025-assignment1-basics/data/owt_bpe_vocab.txt\n",
            "Tokenizers loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import adapters\n",
        "\n",
        "REPO_DIR = \"/content/s2025-assignment1-basics\"\n",
        "DATA_DIR = os.path.join(REPO_DIR, \"data\")\n",
        "\n",
        "TINYSTORIES_VOCAB_PATH = os.path.join(DATA_DIR, \"tinystories_bpe_vocab.txt\")\n",
        "TINYSTORIES_MERGES_PATH = os.path.join(DATA_DIR, \"tinystories_bpe_merges.txt\")\n",
        "\n",
        "OWT_VOCAB_PATH = os.path.join(DATA_DIR, \"owt_bpe_vocab.txt\")\n",
        "OWT_MERGES_PATH = os.path.join(DATA_DIR, \"owt_bpe_merges.txt\")\n",
        "\n",
        "SPECIAL_TOKENS = [\"<|endoftext|>\"]\n",
        "\n",
        "def load_tokenizer(vocab_path, merges_path, special_tokens):\n",
        "    \"\"\"Loads a tokenizer from vocabulary and merges files.\"\"\"\n",
        "    print(f\"Loading tokenizer from {vocab_path} and {merges_path}...\")\n",
        "\n",
        "    try:\n",
        "        with open(vocab_path, \"r\", encoding=\"utf-8\") as vf:\n",
        "            vocab = {int(k): bytes.fromhex(v) for k, v in (line.strip().split() for line in vf)}\n",
        "\n",
        "        with open(merges_path, \"r\", encoding=\"utf-8\") as mf:\n",
        "            merges = [tuple(bytes.fromhex(m) for m in line.strip().split()) for line in mf]\n",
        "\n",
        "        tokenizer = adapters.get_tokenizer(vocab, merges, special_tokens)\n",
        "        print(f\"Tokenizer loaded successfully from {vocab_path}\")\n",
        "\n",
        "        return tokenizer\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Missing tokenizer files: {vocab_path} or {merges_path}\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error while loading tokenizer: {e}\")\n",
        "        return None\n",
        "\n",
        "tiny_tokenizer = load_tokenizer(TINYSTORIES_VOCAB_PATH, TINYSTORIES_MERGES_PATH, SPECIAL_TOKENS)\n",
        "\n",
        "owt_tokenizer = load_tokenizer(OWT_VOCAB_PATH, OWT_MERGES_PATH, SPECIAL_TOKENS)\n",
        "\n",
        "if tiny_tokenizer and owt_tokenizer:\n",
        "    print(\"Tokenizers loaded successfully.\")\n",
        "else:\n",
        "    print(\"Tokenizer loading failed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3KfqtK9kntm",
        "outputId": "a410c8c4-ad59-4ace-b386-8284a9bcb6ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " TinyStories Sample:\n",
            "1: The bird was very thankful to Sam and Fred. They all became friends and learned new things every day. They played and had fun in the quiet forest, and they were always there to help each other.\n",
            "2: Then, something unexpected happened. A big wind came and blew the doll and the car up in the air. The toys landed on the other side of the fence. Now, Lily had Tim's car and Tim had Lily's doll. They both looked at the toys and then at each other. They realized that they could play with the new toys and have fun. So, they decided to share their toys and play together. The stubborn little girl and boy became good friends, and they always shared their toys from that day on.\n",
            "3: As they walked, Clara's mommy said \"These are very expensive, Clara. We don't have enough money to buy them.\"\n",
            "\n",
            " OpenWebText Sample:\n",
            "1: He said that abortion isn’t the main issue at this point, but rather what he sees as a general reluctance to take on issues in the House of Commons.\n",
            "2: Nah, fuck your IP, man I’m sick of y’all geeks.\n",
            "3: In this case, the ranking of reputation was based on asking academics to nominate up to 15 of the \"best\" universities in their field of expertise.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "def fast_sample_lines(file_path, num_samples=10):\n",
        "    file_size = os.path.getsize(file_path)\n",
        "    samples = set()\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        while len(samples) < num_samples:\n",
        "            f.seek(random.randint(0, file_size - 2000))\n",
        "            f.readline()\n",
        "            line = f.readline().strip()\n",
        "            if line:\n",
        "                samples.add(line)\n",
        "\n",
        "    return list(samples)\n",
        "\n",
        "TINYSTORIES_DATA_PATH = \"/content/s2025-assignment1-basics/data/TinyStoriesV2-GPT4-train.txt\"\n",
        "OWT_DATA_PATH = \"/content/s2025-assignment1-basics/data/owt_train.txt\"\n",
        "\n",
        "tiny_samples = fast_sample_lines(TINYSTORIES_DATA_PATH, 10)\n",
        "owt_samples = fast_sample_lines(OWT_DATA_PATH, 10)\n",
        "\n",
        "print(\" TinyStories Sample:\")\n",
        "for i, line in enumerate(tiny_samples[:3]):\n",
        "    print(f\"{i+1}: {line}\")\n",
        "\n",
        "print(\"\\n OpenWebText Sample:\")\n",
        "for i, line in enumerate(owt_samples[:3]):\n",
        "    print(f\"{i+1}: {line}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_compression_ratio(samples, tokenizer):\n",
        "    total_bytes = sum(len(line.encode(\"utf-8\")) for line in samples)\n",
        "    tokenized_samples = [tokenizer.encode(line) for line in samples]\n",
        "    total_tokens = sum(len(tokens) for tokens in tokenized_samples)\n",
        "\n",
        "    return total_bytes / total_tokens\n",
        "\n",
        "tiny_ratio = compute_compression_ratio(tiny_samples, tiny_tokenizer)\n",
        "owt_ratio = compute_compression_ratio(owt_samples, owt_tokenizer)\n",
        "\n",
        "print(f\"TinyStories Tokenizer Compression Ratio: {tiny_ratio:.2f} bytes/token\")\n",
        "print(f\"OpenWebText Tokenizer Compression Ratio: {owt_ratio:.2f} bytes/token\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml19_oSqntA3",
        "outputId": "384a9705-8e7c-4f2e-e0d7-56fe225180f0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TinyStories Tokenizer Compression Ratio: 4.19 bytes/token\n",
            "OpenWebText Tokenizer Compression Ratio: 4.38 bytes/token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"OpenWebText tokenized with its own tokenizer:\", owt_tokenizer.encode(owt_samples[0]))\n",
        "print(\"OpenWebText tokenized with TinyStories tokenizer:\", tiny_tokenizer.encode(owt_samples[0]))\n",
        "\n",
        "\n",
        "\n",
        "owt_with_tiny_ratio = compute_compression_ratio(owt_samples, tiny_tokenizer)\n",
        "\n",
        "print(f\"Compression Ratio of OpenWebText with TinyStories Tokenizer: {owt_with_tiny_ratio:.2f} bytes/token\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujM-QKbGqTIM",
        "outputId": "81163e29-8a31-44c0-c226-d41c3aa16a01"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenWebText tokenized with its own tokenizer: [1083, 482, 326, 20442, 1866, 315, 116, 262, 1281, 1919, 352, 429, 1248, 44, 479, 2252, 601, 340, 8231, 360, 258, 3152, 961, 9707, 577, 283, 1061, 324, 2792, 286, 262, 1979, 287, 7259, 46]\n",
            "OpenWebText tokenized with TinyStories tokenizer: [957, 323, 387, 689, 903, 846, 4294, 1271, 116, 263, 281, 516, 424, 115, 431, 448, 721, 3336, 44, 420, 7266, 564, 285, 1363, 457, 259, 299, 301, 283, 440, 1940, 6362, 1534, 266, 749, 354, 424, 115, 5542, 315, 263, 311, 544, 373, 815, 284, 109, 1754, 46]\n",
            "Compression Ratio of OpenWebText with TinyStories Tokenizer: 3.00 bytes/token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def measure_tokenizer_throughput(tokenizer, text_samples):\n",
        "    total_bytes = sum(len(line.encode(\"utf-8\")) for line in text_samples)\n",
        "\n",
        "    start_time = time.time()\n",
        "    _ = [tokenizer.encode(line) for line in text_samples]\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    throughput = total_bytes / elapsed_time\n",
        "    return throughput\n",
        "\n",
        "tiny_throughput = measure_tokenizer_throughput(tiny_tokenizer, tiny_samples)\n",
        "owt_throughput = measure_tokenizer_throughput(owt_tokenizer, owt_samples)\n",
        "\n",
        "pile_size_bytes = 825 * 1024**3\n",
        "tiny_estimated_time = pile_size_bytes / tiny_throughput\n",
        "owt_estimated_time = pile_size_bytes / owt_throughput\n",
        "\n",
        "print(f\"TinyStories Tokenizer Throughput: {tiny_throughput:.2f} bytes/sec\")\n",
        "print(f\"OpenWebText Tokenizer Throughput: {owt_throughput:.2f} bytes/sec\")\n",
        "print(f\"Estimated time to tokenize the Pile with TinyStories: {tiny_estimated_time / 3600:.2f} hours\")\n",
        "print(f\"Estimated time to tokenize the Pile with OpenWebText: {owt_estimated_time / 3600:.2f} hours\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6mMvKndDvJN",
        "outputId": "c9667550-429c-492b-ef2e-3b3cfd64541e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TinyStories Tokenizer Throughput: 24307.57 bytes/sec\n",
            "OpenWebText Tokenizer Throughput: 19938.01 bytes/sec\n",
            "Estimated time to tokenize the Pile with TinyStories: 10123.01 hours\n",
            "Estimated time to tokenize the Pile with OpenWebText: 12341.54 hours\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def tokenize(input_path, tokenizer, output_path, batch_size=10000):\n",
        "    tokenized_data = []\n",
        "\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as file:\n",
        "        batch = []\n",
        "        for line in file:\n",
        "            batch.append(line.strip())\n",
        "\n",
        "            if len(batch) >= batch_size:\n",
        "                tokenized_data.extend(map(tokenizer.encode, batch))\n",
        "                batch = []\n",
        "\n",
        "        if batch:\n",
        "            tokenized_data.extend(map(tokenizer.encode, batch))\n",
        "\n",
        "    tokenized_array = np.concatenate([np.array(tokens, dtype=np.uint16) for tokens in tokenized_data])\n",
        "    np.save(output_path, tokenized_array)\n",
        "    print(f\"Saved tokenized dataset to {output_path}\")\n",
        "\n",
        "TINYSTORIES_RAW_PATH = \"/content/s2025-assignment1-basics/data/TinyStoriesV2-GPT4-train.txt\"\n",
        "OWT_RAW_PATH = \"/content/s2025-assignment1-basics/data/owt_train.txt\"\n",
        "\n",
        "TINYSTORIES_TOKENS_PATH = \"/content/s2025-assignment1-basics/data/tinystories_tokenized.npy\"\n",
        "OWT_TOKENS_PATH = \"/content/s2025-assignment1-basics/data/owt_tokenized.npy\"\n",
        "\n",
        "tokenize(TINYSTORIES_RAW_PATH, tiny_tokenizer, TINYSTORIES_TOKENS_PATH)\n",
        "tokenize(OWT_RAW_PATH, owt_tokenizer, OWT_TOKENS_PATH)\n",
        "\n",
        "tiny_loaded = np.load(TINYSTORIES_TOKENS_PATH)\n",
        "owt_loaded = np.load(OWT_TOKENS_PATH)\n",
        "\n",
        "print(\"TinyStories tokenized sample:\", tiny_loaded[:10])\n",
        "print(\"OpenWebText tokenized sample:\", owt_loaded[:10])\n"
      ],
      "metadata": {
        "id": "iWUPRgZTFBSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import adapters\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def set_random_seed(seed=0):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "class TokenizedDataset(Dataset):\n",
        "    def __init__(self, file_path, context_length):\n",
        "        self.data = np.memmap(file_path, dtype=np.uint16, mode=\"r\")\n",
        "        self.context_length = context_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.context_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.data[idx: idx + self.context_length]).long()\n",
        "        y = torch.from_numpy(self.data[idx + 1: idx + self.context_length + 1]).long()\n",
        "        return x, y\n",
        "\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, context_length, d_model, num_layers, num_heads, d_ff, attn_pdrop, residual_pdrop):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.context_length = context_length\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.num_heads = num_heads\n",
        "        self.d_ff = d_ff\n",
        "        self.attn_pdrop = attn_pdrop\n",
        "        self.residual_pdrop = residual_pdrop\n",
        "        self.weights = self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        return {k: torch.randn(v.shape, device=device, requires_grad=True) for k, v in adapters.run_transformer_lm.__annotations__.items()}\n",
        "\n",
        "    def forward(self, in_indices):\n",
        "        return adapters.run_transformer_lm(\n",
        "            self.vocab_size,\n",
        "            self.context_length,\n",
        "            self.d_model,\n",
        "            self.num_layers,\n",
        "            self.num_heads,\n",
        "            self.d_ff,\n",
        "            self.attn_pdrop,\n",
        "            self.residual_pdrop,\n",
        "            self.weights,\n",
        "            in_indices\n",
        "        )\n",
        "\n",
        "def train(model, train_loader, val_loader, optimizer, criterion, epochs, checkpoint_path, log_interval):\n",
        "    model.to(device)\n",
        "    best_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x)\n",
        "            loss = criterion(output.view(-1, model.vocab_size), y.reshape(-1))\n",
        "            loss.backward()\n",
        "\n",
        "            adapters.run_gradient_clipping(model.parameters(), max_l2_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % log_interval == 0:\n",
        "                print(f\"Epoch {epoch+1} | Batch {batch_idx} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1} | Avg Loss: {avg_loss:.4f} | Time: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "                output = model(x)\n",
        "                val_loss += criterion(output.view(-1, model.vocab_size), y.reshape(-1)).item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            adapters.run_save_checkpoint(model, optimizer, epoch, checkpoint_path)\n",
        "            print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "\n",
        "\n",
        "class Args:\n",
        "    vocab_size = 10000\n",
        "    context_length = 128\n",
        "    d_model = 256\n",
        "    num_layers = 6\n",
        "    num_heads = 8\n",
        "    d_ff = 1024\n",
        "    attn_pdrop = 0.1\n",
        "    residual_pdrop = 0.1\n",
        "    batch_size = 64\n",
        "    epochs = 10\n",
        "    learning_rate = 3e-4\n",
        "    log_interval = 10\n",
        "    train_path = \"/content/s2025-assignment1-basics/data/tinystories_tokenized.npy\"\n",
        "    val_path = \"/content/s2025-assignment1-basics/data/owt_tokenized.npy\"\n",
        "    checkpoint_path = \"model_checkpoint.pth\"\n",
        "    random_seed = 0\n",
        "\n",
        "args = Args()\n",
        "set_random_seed(args.random_seed)\n",
        "\n",
        "train_data = TokenizedDataset(args.train_path, args.context_length)\n",
        "val_data = TokenizedDataset(args.val_path, args.context_length)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_data, batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "model = TransformerLM(\n",
        "    args.vocab_size, args.context_length, args.d_model,\n",
        "    args.num_layers, args.num_heads, args.d_ff,\n",
        "    args.attn_pdrop, args.residual_pdrop\n",
        ").to(device)\n",
        "\n",
        "optimizer = adapters.AdamWOptimizer(model.parameters(), lr=args.learning_rate)\n",
        "criterion = adapters.run_cross_entropy\n",
        "\n",
        "train(model, train_loader, val_loader, optimizer, criterion, args.epochs, args.checkpoint_path, args.log_interval)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "ELACPyl2LqZg",
        "outputId": "93d2150e-a388-40bc-d373-71b2a11e0ab4"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/s2025-assignment1-basics/data/tinystories_tokenized.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-22dc18002017>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizedDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizedDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-22dc18002017>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, context_length)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTokenizedDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/memmap.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(subtype, filename, dtype, mode, offset, shape, order)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mf_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mf_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'r'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mf_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/s2025-assignment1-basics/data/tinystories_tokenized.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_text(\n",
        "    model, tokenizer, prompt: str, max_tokens: int = 50, temperature: float = 1.0, top_p: float = 0.9, device=\"cuda\"\n",
        ") -> str:\n",
        "    model.eval()\n",
        "\n",
        "    eos_token_id = tokenizer.eos_token_id if hasattr(tokenizer, \"eos_token_id\") else tokenizer.byte_to_id[b\"<|endoftext|>\"]\n",
        "\n",
        "    input_ids = torch.tensor(tokenizer.encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
        "    generated_tokens = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_tokens):\n",
        "            logits = model(input_ids)[:, -1, :]\n",
        "\n",
        "            scaled_logits = logits / temperature\n",
        "            probs = F.softmax(scaled_logits, dim=-1)\n",
        "\n",
        "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "            top_p_mask = cumulative_probs > top_p\n",
        "            top_p_mask[:, 1:] = top_p_mask[:, :-1].clone()\n",
        "            top_p_mask[:, 0] = False\n",
        "\n",
        "            sorted_probs[top_p_mask] = 0\n",
        "            sorted_probs /= sorted_probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "            sampled_index = torch.multinomial(sorted_probs, 1)\n",
        "            next_token = sorted_indices[0, sampled_index].item()\n",
        "\n",
        "            if next_token == eos_token_id:\n",
        "                break\n",
        "\n",
        "            generated_tokens.append(next_token)\n",
        "            input_ids = torch.cat([input_ids, torch.tensor([[next_token]], dtype=torch.long, device=device)], dim=1)\n",
        "\n",
        "    return tokenizer.decode(generated_tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qqrhxjZniKSW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}